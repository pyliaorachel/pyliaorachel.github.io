<!DOCTYPE html>
<html>
	<head>
		<title>穿越時空的偉人：用PyTorch重現偉人們的神經網絡</title>
		<link rel="stylesheet" href="/css/bootstrap.min.css" type="text/css">
		<link rel="stylesheet" href="/css/main.css" type="text/css">
		<link href="https://fonts.googleapis.com/css?family=Antic+Slab|Catamaran|Hind:300|Inconsolata|Josefin+Sans|Muli:300,400i|Poiret+One|Rajdhani:300|Rock+Salt|Ruda|Scope+One|Shadows+Into+Light|Source+Code+Pro|Space+Mono|Amatic+SC:700" rel="stylesheet">
		<link rel="stylesheet" href="/css/github-markdown.css">

		<!-- Place jquery before bootstrap -->
		<script type="text/javascript" src="/js/jquery.min.js"></script>
		<script type="text/javascript" src="/js/bootstrap.min.js"></script>
	</head>
	<body>
		<nav class="navbar navbar-default navbar-fixed-top">
	<div class="container-fluid">

		<div class="navbar-header">
			<button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse" aria-expanded="false">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			<a class="navbar-brand" href="/">MyCoon</a>
		</div>

		<div class="collapse navbar-collapse" id="navbar-collapse">

			<form class="navbar-form navbar-left">
				<div class="form-group">
					<input type="text" class="form-control" placeholder="Search">
				</div>
				<button type="submit" class="btn btn-default">Submit</button>
			</form>

			<ul class="nav navbar-nav navbar-right" id="nav-links">
				<li class="nav-home"><a href="/">Home</a></li>
				<li class="dropdown nav-blog">
					<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
						Blog<span class="caret"></span>
					</a>
					<ul class="dropdown-menu">
						<li><a href="/blog">MyCoon Blog</a></li>
						<li role="separator" class="divider"></li>
						<li><a href="/blog/categories">Categories</a></li>
						
							
						

						
							
							
							<li class="category">
								<a href="/blog/tech">
									<span>Tech</span>
									<span>11</span>
								</a>
							</li>
						
							
							
							<li class="category">
								<a href="/blog/notes">
									<span>Notes</span>
									<span>5</span>
								</a>
							</li>
						
						<li><a href="/blog/tags">Tags</a></li>
					</ul>
				</li>
				<li class="dropdown nav-tutorial">
					<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
						Tutorials<span class="caret"></span>
					</a>
					<ul class="dropdown-menu">
						<li><a href="/tutorial">MyCoon Tutorial</a></li>
						<li role="separator" class="divider"></li>
						<li><a href="/tutorial/categories">Categories</a></li>
						
							
						

						
							
							
							<li class="category">
								<a href="/tutorial/devops">
								<span>DevOps</span>
								<span>1</span></a>
							</li>
						
							
							
							<li class="category">
								<a href="/tutorial/hardware">
								<span>Hardware</span>
								<span>1</span></a>
							</li>
						
						<li><a href="/tutorial/tags">Tags</a></li>
					</ul>
				</li>
				<li  class="nav-project"><a href="/project">Projects</a></li>
				<li class="nav-special" id="special"><a href="#">?</a></li>
			</ul>
		</div><!-- /.navbar-collapse -->
	</div><!-- /.container-fluid -->
</nav>
		<main class="page-content post" aria-label="Content">
	<div class="post-wrapper">
		<div class="wrapper">
	<article class="post-article" itemscope itemtype="http://schema.org/BlogPosting">

		<header class="post-header">
			<h1 class="post-title" itemprop="name headline">穿越時空的偉人：用PyTorch重現偉人們的神經網絡</h1>
			<p class="post-meta">Dec 24, 2017
				 • Tech
				 • pyliaorachel
			</p>
		</header>

		

		<span class="separator"> • • • </span>

		<div class="post-content" itemprop="articleBody">
			<p>繼上一篇<a href="https://medium.com/pyladies-taiwan/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E6%96%B0%E6%89%8B%E6%9D%91-pytorch%E5%85%A5%E9%96%80-511df3c1c025">深度學習新手村：PyTorch 入門</a>後，這一次要來做一點進階應用。筆者今年十一月參與在香港舉辦的 PyCon，其中 Aditthya Ramakrishnan 講者演講的主題 <a href="https://www.youtube.com/watch?v=r8H1cZjCfIA">Resurrecting the dead with deep learning</a> 以 RNN 模型訓練林肯 (Lincoln) 及希特勒 (Hitler)的混合語料庫，創造出講話非常矛盾的林克勒 (Lincler)。</p>

<p>以此演講為基礎，這次收集並混合了《毛澤東語錄》和《論語》，嘗試模擬出一個「孔澤東」，藉此一窺 RNN 在中文文本生成 (Chinese text generation) 的應用。</p>

<!--more-->

<hr>

<p><em>* 請注意，此篇 PyTorch 建立在 Python3 之上，並以 MacOSX 為環境。</em><br>
<em>* 預備知識：基礎神經網絡概念</em></p>

<p>人工神經網絡 (artificial neural network) 隨其不同的架構有著不同的應用，其中<strong>循環神經網絡 (recurrent neural network, RNN)</strong> 能捕捉<strong>時間</strong>關係，在自然語言處理領域有著廣泛的應用。本文將以簡介 RNN 及其優勢開頭，再進入主專案介紹，按照步驟講解如何以 PyTorch 進行中文文本生成，將歷史人物玩弄於股掌間，打造出一個荒謬的偉人結合體，一同維護世界和平。</p>

<h2>RNN （相當簡單的）介紹</h2>

<p>還記得 N 年前的 Google 翻譯嗎？翻譯的結果除了相當生硬不精確，還經常被眾人在茶餘飯後拿來揶揄，令人鼻酸。但 Google 在 2016 年將其打掉重練，推出了一個<a href="https://research.google.com/pubs/pub45610.html">新系統</a>，有嘗試過的應該都會驚艷於它的成長，流暢度與精確度都提升許多，一種小孩長大的感動。這個新系統即是建立在一種稱之為<strong>序列到序列 (sequence to sequence, seq2seq)</strong> 的模型之上，而此種模型便是以 RNN 為基礎。</p>

<p><strong>循環神經網絡 (RNN)</strong> 旨在建立一種<strong>記憶</strong>，也就是為了不將先前輸出的結果遺忘，將之累積成某種隱藏狀態 (hidden state)，並與當前輸入結合，一起產出結果，再進一步傳遞下去。也因此，RNN 適合接收序列 (sequence) 作為輸入並輸出序列，提供了序列生成一個簡潔的模型。</p>

<p><img src="https://karpathy.github.io/assets/rnn/diags.jpeg" alt="RNN"></p>

<p>最原始的 RNN 有其限制，學者為了突破這些限制而發展出了一些變形，其中廣泛應用的<strong>長短期記憶 (Long Short-Term Memory, LSTM)</strong> 即是為了解決 <a href="http://harinisuresh.com/2016/10/09/lstms/">vanishing gradient</a> 問題而提出，也是我們接下來實作中應用的模型。</p>

<p>礙於篇（本）幅（人）有（太）限（懶），沒辦法完整解釋這些模型背後的原理，但想要應用或覺得生命有限的話，不妨就將之視為黑盒子。若有興趣進一步了解，可以膜拜一下<a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">這篇詳盡介紹</a>和<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">這篇</a>。</p>

<h2>以 PyTorch 重現偉人們的神經網絡</h2>

<p>今年十一月的 PyCon HK 的其中一場演講 <a href="https://www.youtube.com/watch?v=r8H1cZjCfIA">Resurrecting the dead with deep learning</a> 將林肯 (Lincoln) 及希特勒 (Hitler) 的語料結合，進行訓練後能打造一個自打嘴巴的文本生成系統，稱之為林克勒 (Lincler)。此次專案則是仿造其精髓，但將文本改成中文，並以 PyTorch 實現（原專案以 Keras 實現）。</p>

<p>如果跟筆者一樣也是 PyTorch 新手，就一起來邊玩邊練習吧！</p>

<p>GitHub 專案原始碼：<a href="https://github.com/pyliaorachel/resurrecting-the-dead-chinese">pyliaorachel/resurrecting-the-dead-chinese</a></p>

<p><em>* 以下會簡單提到很多深度學習的概念，皆當作補充即可。欲深入了解可參考提供的連結。</em>  </p>

<h4>語料準備</h4>

<p>這次準備的兩個歷史人物的語料，一是毛澤東的《毛澤東語錄》，一是孔子與其弟子的《論語》。原本是想找蔣中正的《總統蔣公思想言論總集》，但找不到公開的語料，真是可惜。</p>

<p>資料清理方面，只將原始語料中的一些非人物言論的註解刪除後，一句句排好。另外由於《論語》原文是文言文，所以挑了白話文翻譯，避免結果文白混雜。繁簡轉換方面，原始語料皆為簡體中文，所以不需進行繁簡轉換；如果想自己準備語料進行訓練，可以使用 <a href="https://github.com/BYVoid/OpenCC">OpenCC</a> 將繁簡統一。</p>

<p>以上清理都相當簡單，只透過文字編輯器的 find &amp; replace 就可以完成（很懶惰我知道）。混合語料則簡單寫了 <a href="https://github.com/pyliaorachel/resurrecting-the-dead-chinese/blob/master/src/corpus/mix.py">python script</a> 把兩個檔案中的句子隨機混排。</p>

<p>原始和清理後的語料都在 <a href="https://github.com/pyliaorachel/resurrecting-the-dead-chinese/tree/master/corpus">corpus 檔案夾</a>底下。</p>

<h4>建立模型</h4>

<h6>輸入/輸出資料</h6>

<p>簡單複習一下監督式學習。一般監督式學習的訓練過程中，每一筆資料都需要包成<code>(input, target)</code>的形式；<code>input</code> 進入模型後會得到一個預測 <code>output</code>，而這個 <code>output</code> 和我們的正解 <code>target</code> 之間會有一個落差 (error)。為了讓落差減小，我們需要慢慢調整模型中參數，最後達到準確的預測，這個就是模型的學習過程。</p>

<p>這次的任務中，我們讓 <code>input</code> 為一序列的中文字，<code>target</code> 則是此序列後的下一個中文字，兩者皆從語料中準備即可。這邊簡單起見，直接以中文字為單位而不再做中文分詞，如果想以詞為單位可以使用<a href="https://github.com/fxsjy/jieba">結巴分詞</a>。</p>

<p>假設輸入序列長度為 5，則<code>这正是我们弟子们学不到的。</code>會被包成：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># (input, target)</span>
<span class="p">(</span><span class="s">'这正是我们'</span><span class="p">,</span> <span class="s">'弟'</span><span class="p">)</span>
<span class="p">(</span><span class="s">'正是我们弟'</span><span class="p">,</span> <span class="s">'子'</span><span class="p">)</span>
<span class="p">(</span><span class="s">'是我们弟子'</span><span class="p">,</span> <span class="s">'们'</span><span class="p">)</span>
<span class="p">(</span><span class="s">'我们弟子们'</span><span class="p">,</span> <span class="s">'学'</span><span class="p">)</span>
<span class="p">(</span><span class="s">'们弟子们学'</span><span class="p">,</span> <span class="s">'不'</span><span class="p">)</span>
<span class="p">(</span><span class="s">'弟子们学不'</span><span class="p">,</span> <span class="s">'到'</span><span class="p">)</span>
<span class="p">(</span><span class="s">'子们学不到'</span><span class="p">,</span> <span class="s">'的'</span><span class="p">)</span>
<span class="p">(</span><span class="s">'们学不到的'</span><span class="p">,</span> <span class="s">'。'</span><span class="p">)</span>
</code></pre></div>
<p>另外就是，一筆一筆資料輸入後即更新權重，會讓訓練變得很慢。多筆資料包在一起一起訓練，可以加速訓練，此方法稱之為 <a href="https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/">mini-batch</a>。那為什麼不所有資料包成一筆呢？因為這樣一來收斂結果會比較差，而且每次有新資料進來就要整包重新訓練一次；mini-batch 算是一個平衡點，不過 batch size 要多大就需要調校一番。</p>

<p><a href="https://github.com/pyliaorachel/resurrecting-the-dead-chinese/blob/master/src/train/data.py"><code>src/train/data.py</code></a>裡有兩個 function 負責準備好模型可以接受的 input：</p>

<p><code>parse_corpus</code></p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># 語料裡所有出現過的中文字，此為 vocabulary</span>
<span class="n">chars</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">raw_text</span><span class="p">)))</span>

<span class="c"># 給每個中文字一個對應的 index，比較好做接下來的任務</span>
<span class="n">char_to_int</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">c</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">))</span>
<span class="n">int_to_char</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">))</span>

<span class="c"># 共生成 N 個 input-target pair，每個 input 長度為 seq_length，target 長度為 1</span>
<span class="n">n_chars</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">raw_text</span><span class="p">)</span>
<span class="n">dataX</span> <span class="o">=</span> <span class="p">[]</span> <span class="c"># N x seq_length</span>
<span class="n">dataY</span> <span class="o">=</span> <span class="p">[]</span> <span class="c"># N x 1</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_chars</span> <span class="o">-</span> <span class="n">seq_length</span><span class="p">):</span>
    <span class="n">seq_in</span> <span class="o">=</span> <span class="n">raw_text</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">seq_length</span><span class="p">]</span>
    <span class="n">seq_out</span> <span class="o">=</span> <span class="n">raw_text</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">seq_length</span><span class="p">]</span>
    <span class="n">dataX</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">char_to_int</span><span class="p">[</span><span class="n">char</span><span class="p">]</span> <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">seq_in</span><span class="p">])</span>
    <span class="n">dataY</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">char_to_int</span><span class="p">[</span><span class="n">seq_out</span><span class="p">])</span>
</code></pre></div>
<p><code>format_data</code></p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># 採用 mini-batch，尾巴不足 batch_size 的直接捨棄</span>
<span class="n">n_patterns</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataY</span><span class="p">)</span>
<span class="n">n_patterns</span> <span class="o">=</span> <span class="n">n_patterns</span> <span class="o">-</span> <span class="n">n_patterns</span> <span class="o">%</span> <span class="n">batch_size</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">dataX</span><span class="p">[:</span><span class="n">n_patterns</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">dataY</span><span class="p">[:</span><span class="n">n_patterns</span><span class="p">]</span>

<span class="c"># 把 array 每 batch_size 筆資料包成一組，並包成 tensor</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

<span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</code></pre></div>
<h6>LSTM 模型</h6>

<p>PyTorch 建立 NN 的話需要繼承 <code>nn.Module</code>，並 override <code>__init__</code> 和 <code>forward</code> 兩個 method。<a href="https://github.com/pyliaorachel/resurrecting-the-dead-chinese/blob/master/src/train/model.py"><code>src/train/model.py</code></a>定義了我們的 NN 架構。</p>

<p>值得一提的是，輸入的每個中文字都會先轉成 embedding vector，也就是用一個 vector 來表示各個中文字，這在自然語言處理任務中幾乎是必要的處理。<a href="https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12">這篇文章</a>對 embedding vector 有一個很好的介紹，不過簡單來說，因為字詞是類別資料 (categorical data)，用 integer 這種有順序的格式來表示並不恰當，因此轉成 vector 形式，藉由 vector 之間的空間關係來捕捉字詞之間的關聯性。</p>

<p>Dropout 則是常見的防止<strong>過擬合 (overfitting)</strong> 的手段，也就是在訓練過程中三不五時捨棄/忽略一些神經元，來減弱他們彼此間的聯合適應性 (co-adaptation)。不能說太多，不然要變 DLadies (DeepLearningLadies) 了，詳可參考<a href="https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5">此篇</a>。</p>

<p>這邊設計的架構總共有以下幾層：</p>

<ol>
<li>Embedding layer: 將以 integer 表示的 character index 轉成 embedding vector</li>
<li>LSTM layer + dropout: 將輸入序列通過 LSTM 編碼成 hidden state，並加一層 dropout 防止 overfitting</li>
<li>Fully-connected layer: 把 hidden state 線性轉換成一長度為 length of vocabulary 的 output layer，其中數值當作每個字的得分，得分越高越有機會是下一個預測結果</li>
</ol>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_vocab</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="n">embedding_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>

        <span class="c"># nn.Embedding 可以幫我們建立好字典中每個字對應的 vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">n_vocab</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>

        <span class="c"># LSTM layer，形狀為 (input_size, hidden_size, ...)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

        <span class="c"># Fully-connected layer，把 hidden state 線性轉換成 output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden2out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_vocab</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_in</span><span class="p">):</span>
        <span class="c"># LSTM 接受的 input 形狀為 (timesteps, batch, features)，即 (seq_length, batch_size, embedding_dim)</span>
        <span class="c"># 所以先把形狀為 (batch_size, seq_length) 的 input 轉置後，再把每個 value (char index) 轉成 embedding vector</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">seq_in</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>

        <span class="c"># LSTM 層的 output (lstm_out) 有每個 timestep 出來的結果（也就是每個字進去都會輸出一個 hidden state）</span>
        <span class="c"># 這邊我們取最後一層的結果，即最近一次的結果，來預測下一個字</span>
        <span class="n">lstm_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="n">ht</span> <span class="o">=</span> <span class="n">lstm_out</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c"># 線性轉換至 output</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden2out</span><span class="p">(</span><span class="n">ht</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
<h4>訓練模型</h4>

<p>資料和模型都有了之後，就可以來訓練了。<a href="https://github.com/pyliaorachel/resurrecting-the-dead-chinese/blob/master/src/train/train.py"><code>src/train/train.py</code></a>負責載入資料、訓練、及儲存結果。</p>

<p>Optimizer 選用 <a href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/">Adam</a>，亦可調用其他如 SGD、RMSprop 等 <a href="http://pytorch.org/docs/master/optim.html">optimizer</a>。</p>

<p>Loss function 採用的是 classification 任務常見的 cross-entropy。預測的 <code>output</code> 會是長度為 number of classes 的 tensor，<code>target</code> 則是正確 class label，而 PyTorch 裡的 <code>cross_entropy</code> 會負責把預測結果做一次 log softmax 後，計算跟目標之間的 negative log likelihood，因此預測結果不需要先做 softmax 或 log softmax。需要特別注意的是，不同的深度學習框架會有不同的參數形狀要求，例如 Keras 會需要你把 target 轉成 one-hot encoding 等。</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">log_interval</span><span class="p">):</span>
    <span class="c"># 設一下 flag</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="c"># Mini-batch 訓練 </span>
    <span class="k">for</span> <span class="n">batch_i</span><span class="p">,</span> <span class="p">(</span><span class="n">seq_in</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
        <span class="n">seq_in</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">seq_in</span><span class="p">),</span> <span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">seq_in</span><span class="p">)</span>                      <span class="c"># 取得預測</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>      <span class="c"># 計算 loss</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>                             <span class="c"># Backpropagation</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>                            <span class="c"># 更新參數</span>

        <span class="c"># Log 訓練進度</span>
        <span class="k">if</span> <span class="n">batch_i</span> <span class="o">%</span> <span class="n">log_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'Train epoch: {} ({:2.0f}</span><span class="si">%</span><span class="s">)</span><span class="se">\t</span><span class="s">Loss: {:.6f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">batch_i</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div><div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># 載入資料，建立模型</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">dataX</span><span class="p">,</span> <span class="n">dataY</span><span class="p">,</span> <span class="n">char_to_int</span><span class="p">,</span> <span class="n">int_to_char</span><span class="p">,</span> <span class="n">chars</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">corpus</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">),</span> <span class="n">args</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>

<span class="c"># 訓練</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">log_interval</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">log_interval</span><span class="p">)</span>

    <span class="c"># 為避免不可抗力因素造成訓練中斷，或訓練太久失去耐心，每幾個 epoch 就儲存一次模型</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">args</span><span class="o">.</span><span class="n">save_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>
</code></pre></div>
<h4>產出結果</h4>

<p>訓練好模型後，接下來就來試試看生成文本。方法是，從語料中隨機選一個序列作為開端，輸入模型得到下一個字後，將之附在序列末，並將原序列頭一個字移除，以此新序列繼續進行預測，直到句子結束。<a href="https://github.com/pyliaorachel/resurrecting-the-dead-chinese/blob/master/src/generate_text/gen.py"><code>src/generate_text/gen.py</code></a>負責文本生成。</p>

<p>但模型給出的 output 是一個長度為 length of vocabulary 的分數 vector，要怎麼挑選下一個字呢？第一直覺是，選分數最高的，即是我寫的<a href="https://github.com/pyliaorachel/resurrecting-the-dead-chinese/blob/f0cff5a5a100957a42f0a24c3e7b1b25a0a75d86/src/generate_text/gen.py">第一個版本</a>。但生成的結果很悲劇（十句）：</p>
<div class="highlight"><pre><code class="language-" data-lang="">数人民的政策，而不是为了这个人民的，是一个人民的工作，我们的工作是一个人的一个具体的工作，我们的工作是一个人，这是一个人的时候，我们就要使他们在革命中国的人民主要关系，不是要经过这种情况，在社会主义制度和国家政治工作作风，不能用正确的方法去解决。这是一个人民的政策，而是在全国的领导机关，不是为着我们的民主主义，是一个革命的政策，而是在全国的人民主要的，是在革命的政治上，在一个人民内部的矛盾，是一个人民的工作，我们的工作是一个人的一个具体的工作，我们的工作是一个人，这是一个人口作为一个革命的政策，而是在全国的领导机关，不是为着我们的民主主义，是一个革命的政策，而是在全国的人民主要的，是在革命的政治上，在一个人民内部的矛盾，是一个人民的工作，我们的工作是一个人的一个具体的工作，我们的工作是一个人，这是一个人口作为一个革命的政策...
</code></pre></div>
<p>會發生一直重複的情況，而且產生不了句號，所以句子停不下來。大概是只要接近序列末的那幾個字相似，產出來的分數分佈也相似，因此分數最高的很可能都是同一個字。</p>

<p>為了避免這種事發生，第二個版本（也就是以下的版本）將分數 vector 轉成機率分佈，並依照此分佈挑選下一個字。例如 vocabulary 裡有三個字 <code>[&#39;你&#39;, &#39;我&#39;, &#39;他&#39;]</code>，而機率分佈是 <code>[0.8, 0.1, 0.1]</code>，則挑選十次之中，理想中會有 8 次挑 &#39;你&#39;，各 1 次挑 &#39;我&#39; 和 &#39;他&#39;，而非總是挑 &#39;你&#39; 了。</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># 隨機選擇一序列作為開端</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_patterns</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">pattern</span> <span class="o">=</span> <span class="n">patterns</span><span class="p">[</span><span class="n">start</span><span class="p">]</span>

<span class="c"># 共 n_sent 句子要生成</span>
<span class="n">cnt</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="n">cnt</span> <span class="o">&lt;</span> <span class="n">n_sent</span><span class="p">:</span> 
    <span class="c"># 包一下 input</span>
    <span class="n">seq_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pattern</span><span class="p">)</span>
    <span class="n">seq_in</span> <span class="o">=</span> <span class="n">seq_in</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c"># batch_size = 1</span>

    <span class="n">seq_in</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">seq_in</span><span class="p">))</span>

    <span class="c"># 生成此序列下一個字</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">seq_in</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">to_prob</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="c"># softmax 後轉成機率分佈</span>
    <span class="n">char</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">chars</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pred</span><span class="p">)</span>          <span class="c"># 依機率分佈選字</span>
    <span class="n">char_idx</span> <span class="o">=</span> <span class="n">char_to_int</span><span class="p">[</span><span class="n">char</span><span class="p">]</span>

    <span class="c"># 印出</span>
    <span class="k">print</span><span class="p">(</span><span class="n">char</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s">''</span><span class="p">)</span>

    <span class="c"># 將字附在原序列後並移除第一個字，作為下一個 input 序列</span>
    <span class="n">pattern</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">char_idx</span><span class="p">)</span>
    <span class="n">pattern</span> <span class="o">=</span> <span class="n">pattern</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="c"># 若印出代表句子結尾的標點符號，則完成一個句子生成</span>
    <span class="k">if</span> <span class="n">is_end</span><span class="p">(</span><span class="n">char</span><span class="p">):</span>
        <span class="c"># restart_seq 決定要不要重新挑選一個序列，或是完成一個完整段落</span>
        <span class="k">if</span> <span class="n">restart_seq</span><span class="p">:</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_patterns</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">pattern</span> <span class="o">=</span> <span class="n">patterns</span><span class="p">[</span><span class="n">start</span><span class="p">]</span>
            <span class="k">print</span><span class="p">()</span>

        <span class="n">cnt</span> <span class="o">+=</span> <span class="mi">1</span> 
</code></pre></div>
<p>生成的結果有大幅度的改善，下面是一些結果（參數：sequence length = 50, batch size = 32, embedding dimension = 256, hidden dimension = 256, learning rate = 0.0001, dropout = 0.2, epoch = 30; 訓練時間：5~6小時 on 2 GHz Intel Core i5 CPU）：</p>
<div class="highlight"><pre><code class="language-" data-lang="">在战争中否坚决地实行和全体生活的形势是错误的，那一世界有特点的人们想不另一时，愿意的指导、非英时、半成大党的估计。按照这个人，若不重视显极，而是使他们取得胜利。我也一定要君子使国家吗？可以白有的大子加制，使使他服长。难道这怎么能短良呢？则我早上怎这么样？说得到太的觉多了没才会名季厌。恶—那样里来不会自六方），不奏怨恨，我敢不敬，言语他的推子。君子恭敬仁义。帝国主义已经：说多了，不可见变愿意十五，即后，一战不能打干，就自以只是在那类里的自线之养。
</code></pre></div><div class="highlight"><pre><code class="language-" data-lang="">智的诸侯了，天下的交朝吗？有礼有制度，然后却是殷容易听了。马克思列宁主义者，要学习端木不是教别的方法，不可以说出发死的。了符合的主义形规定的就是无不理的，还有一问当，最简如见同同志同民族领导、干部、活化，相间工互来关头党的整风。但是还每一个整一个新干部，要看教育内部同统权力更等、干部其、活庭和巩固群众。这不是尊重那个问题，就要说弄个根据，不作任何由统一的斗争。但是在我国现在资本主义的剥削和共产党，不是很好地接受社会主义的工作方向。每一个基本作用外，结果有一个专于战役它。党的政治任务对于行动；（要有几先之礼，几然年，不允动地也会必要要使拒主观愿意。十岁伟大还有中国历史这种具体这要的。
</code></pre></div><div class="highlight"><pre><code class="language-" data-lang="">争，我们会主义和平国，而没有把国的人和厌恶。这样的人不宽易。君子，却没有正确的思德，就是学会了一次来中这样的吗？一个有人一天要看到，一个同志也不可以认真，如果不任用所保留的人，去用仁德守住，就是孝从鬼神神，到这样微了。在练兵，都思考情齐智百了，就不说，该容易作等；学斗争取青，不给以那就不去，全年的才能。以发挥为杀王的不能不会得彻底，他来不可以做；不而不上与，拿着我们，也将他打民主观。一定要说是不安的；言论周别的人对亲下，不吃肉，不吃。当然而推举，拿受蒋介石的圣人。宰查回果是什么事了，可以让他自己高兴的事，小人批评。不说话，这是反而不是孝悌。
</code></pre></div>
<p>雖說生成的句子看起來比較不鬼打牆了，也看得出學習到了一些完整的詞，例如「马克思列宁主义」、「蒋介石」、「君子恭敬仁义」等，不過離正確文法還有不小差距。但因為資源有限，我沒有進行調參，有興趣的人可以試試看能不能把模型訓練的更好。</p>

<p>分享一下演講裡 Lincler 的一些結果：</p>
<div class="highlight"><pre><code class="language-" data-lang="">In 1918, I am the warfare. The struggle of civilization. The only answer to absolute liberty is the destruction of the nations.
</code></pre></div><div class="highlight"><pre><code class="language-" data-lang="">In the wrong virtue of people, to control every point the intention of love is to demand the supremacy of the United States.
</code></pre></div><div class="highlight"><pre><code class="language-" data-lang="">The British people will be sad with the progressing of the United States. Distrust the economy.
</code></pre></div>
<p>據說已經是篩選其中比較好的句子生成了。有興趣的可以聽聽看這場<a href="https://www.youtube.com/watch?v=r8H1cZjCfIA">演講</a>（溫馨提醒：是印度腔喔）。</p>

<h2>結語</h2>

<p>Seq2seq 模型為文本生成提供了簡單有效的方法，也為自然語言處理界注入了更多可能性。此篇以《毛澤東語錄》和《論語》為語料，嘗試打造出矛盾的文本生成系統，雖說離可被理解的語言還有一大段差距，卻也不難看出 RNN 和 seq2seq 的潛力。此外，PyTorch 的實作相當好上手且簡單易懂，唯其剛剛崛起，網路上能找到的實例不如 TensorFlow 多，對新手來說挑戰頗多，期望未來社群發展能夠更健全囉。</p>

<h2>參考資料</h2>

<ol>
<li><a href="https://www.youtube.com/watch?v=r8H1cZjCfIA">PyCon HK 2017 - Resurrecting the dead with deep learning</a></li>
<li><a href="https://research.google.com/pubs/pub45610.html">Google&#39;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</a></li>
<li><a href="https://www.zhinengl.com/2017/01/sequence-to-sequence-learning/">谷歌翻译背后的技术突破：序列到序列学习</a></li>
<li><a href="http://harinisuresh.com/2016/10/09/lstms/">Vanishing Gradients &amp; LSTMs</a></li>
<li><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></li>
<li><a href="https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/">A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size</a></li>
<li><a href="https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12">Deep Learning #4: Why You Need to Start Using Embedding Layers</a></li>
<li><a href="https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5">Dropout in (Deep) Machine learning</a></li>
<li><a href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/">Gentle Introduction to the Adam Optimization Algorithm for Deep Learning</a></li>
</ol>

		</div>

		<ul class="tag-list">
			

			<li class="tag">
				Python
			</li>

			

			<li class="tag">
				PyTorch
			</li>

			

			<li class="tag">
				Machine Learning
			</li>

			

			<li class="tag">
				Deep Learning
			</li>

			

			<li class="tag">
				Neural Network
			</li>

			

			<li class="tag">
				Natural Language Processing
			</li>

			

			<li class="tag">
				PyLadies
			</li>

			
		</ul>

	</article>

	
		<div id="disqus_thread"></div>
<script>
	var disqus_config = function () {
        this.page.url = https://pyliaorachel.github.io/blog/tech/nlp/2017/12/24/resurrecting-the-dead-chinese.html;
        this.page.identifier = /blog/tech/nlp/2017/12/24/resurrecting-the-dead-chinese;
	};
	(function() {
	    var d = document, s = d.createElement('script');
	    s.src = '//pyliaorachel.disqus.com/embed.js';
	    s.setAttribute('data-timestamp', +new Date());
	    (d.head || d.body).appendChild(s);
	})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

	
</div>
	</div>
</main>
		<footer>
	<div class="container-fluid">
		<div class="row">
			<div class="col-md-4 footer-link">
				<a href="mailto:rachel5566kk@gmail.com">
					<span class="icon icon-email">
						<img src="/assets/svg/email-logo.svg" alt="Email Logo">
					</span>
					<span class="username">rachel5566kk@gmail.com</span>
				</a>
			</div>
			<div class="col-md-4 footer-link">
				<a href="https://github.com/pyliaorachel"  target="_blank">
					<span class="icon icon-github">
						<img src="/assets/svg/github-logo.svg" alt="GitHub Logo">
					</span>
					<span class="username">pyliaorachel</span>
				</a>
			</div>
			<div class="col-md-4 footer-link">
				<a href="https://tw.linkedin.com/in/peiyu-liao" target="_blank">
					<span class="icon icon-linkedin">
						<img src="/assets/svg/linkedin-logo.svg" alt="LinkedIn Logo">
					</span>
					<span class="username">Peiyu Liao</span>
				</a>
			</div>
		</div><!-- /.row -->
		<div class="row">
			<div class="footer-copy-right">
				<p>2018	&copy; Liao Peiyu</p>
			</div >
		</div><!-- /.row -->
	</div><!-- /.container-fluid -->
</footer>

		<script type="text/javascript" src="/js/main.js"></script>
		<script type="text/javascript" src="/js/home.js"></script>
		<script id="dsq-count-scr" src="//pyliaorachel.disqus.com/count.js" async></script>
	</body>
</html>