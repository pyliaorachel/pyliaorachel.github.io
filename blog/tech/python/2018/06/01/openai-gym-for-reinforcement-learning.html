<!DOCTYPE html>
<html>
	<head>
		<title>Reinforcement Learning 健身房：OpenAI Gym</title>
		<link rel="stylesheet" href="/css/bootstrap.min.css" type="text/css">
		<link rel="stylesheet" href="/css/main.css" type="text/css">
		<link href="https://fonts.googleapis.com/css?family=Antic+Slab|Catamaran|Hind:300|Inconsolata|Josefin+Sans|Muli:300,400i|Poiret+One|Rajdhani:300|Rock+Salt|Ruda|Scope+One|Shadows+Into+Light|Source+Code+Pro|Space+Mono|Amatic+SC:700" rel="stylesheet">
		<link rel="stylesheet" href="/css/github-markdown.css">

		<!-- Place jquery before bootstrap -->
		<script type="text/javascript" src="/js/jquery.min.js"></script>
		<script type="text/javascript" src="/js/bootstrap.min.js"></script>
	</head>
	<body>
		<nav class="navbar navbar-default navbar-fixed-top">
	<div class="container-fluid">

		<div class="navbar-header">
			<button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse" aria-expanded="false">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			<a class="navbar-brand" href="/">MyCoon</a>
		</div>

		<div class="collapse navbar-collapse" id="navbar-collapse">

			<form class="navbar-form navbar-left">
				<div class="form-group">
					<input type="text" class="form-control" placeholder="Search">
				</div>
				<button type="submit" class="btn btn-default">Submit</button>
			</form>

			<ul class="nav navbar-nav navbar-right" id="nav-links">
				<li class="nav-home"><a href="/">Home</a></li>
				<li class="dropdown nav-blog">
					<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
						Blog<span class="caret"></span>
					</a>
					<ul class="dropdown-menu">
						<li><a href="/blog">MyCoon Blog</a></li>
						<li role="separator" class="divider"></li>
						<li><a href="/blog/categories">Categories</a></li>
						
							
						

						
							
							
							<li class="category">
								<a href="/blog/tech">
									<span>Tech</span>
									<span>11</span>
								</a>
							</li>
						
							
							
							<li class="category">
								<a href="/blog/notes">
									<span>Notes</span>
									<span>5</span>
								</a>
							</li>
						
						<li><a href="/blog/tags">Tags</a></li>
					</ul>
				</li>
				<li class="dropdown nav-tutorial">
					<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
						Tutorials<span class="caret"></span>
					</a>
					<ul class="dropdown-menu">
						<li><a href="/tutorial">MyCoon Tutorial</a></li>
						<li role="separator" class="divider"></li>
						<li><a href="/tutorial/categories">Categories</a></li>
						
							
						

						
							
							
							<li class="category">
								<a href="/tutorial/devops">
								<span>DevOps</span>
								<span>1</span></a>
							</li>
						
							
							
							<li class="category">
								<a href="/tutorial/hardware">
								<span>Hardware</span>
								<span>1</span></a>
							</li>
						
						<li><a href="/tutorial/tags">Tags</a></li>
					</ul>
				</li>
				<li  class="nav-project"><a href="/project">Projects</a></li>
				<li class="nav-special" id="special"><a href="#">?</a></li>
			</ul>
		</div><!-- /.navbar-collapse -->
	</div><!-- /.container-fluid -->
</nav>
		<main class="page-content post" aria-label="Content">
	<div class="post-wrapper">
		<div class="wrapper">
	<article class="post-article" itemscope itemtype="http://schema.org/BlogPosting">

		<header class="post-header">
			<h1 class="post-title" itemprop="name headline">Reinforcement Learning 健身房：OpenAI Gym</h1>
			<p class="post-meta">Jun 1, 2018
				 • Tech
				 • pyliaorachel
			</p>
		</header>

		

		<span class="separator"> • • • </span>

		<div class="post-content" itemprop="articleBody">
			<p>不久前火熱的 AlphaGo 圍棋 AI 系統因打敗眾多人類好手而聲名大噪，而稍後推出的進化版 AlphaGo Zero 更是乾淨利落的藉由與自身對抗而習得棋藝，令人嘖嘖稱奇。而這系列圍棋 AI 系統背後即是以 <strong>Reinforcement Learning 強化學習</strong>為基礎訓練而成。</p>

<p><code>Gym</code> 是 OpenAI 所開源的 Reinforcement Learning 工具包。無論是想感受 Reinforcement Learning 是怎麼一回事，或是想嘗試進階 Deep Q-Learning 的開發者，都可以快速方便的調用 <code>gym</code> 所提供的許多現成環境，專注於演算法的設計與實現。快讓我們一起來成為健身房的永久免費會員！</p>

<!--more-->

<hr>

<p><em>* 請注意，以下只針對Python3進行講解與測試，並以 MacOSX 為環境。</em></p>

<p>本篇會從基礎 Reinforcement Learning 概念簡介開始，進入 OpenAI gym 簡介，跟著兩個 demo 式的簡單演算法實作 -- <strong>Random Action</strong> 及 <strong>Hand-Made Policy</strong>，最後帶至具有學習能力的演算法 --  <strong>Q table 為基礎的 Q-learning</strong>。與 Deep Learning 結合的 Deep Q-learning 會在之後的進階篇實作。</p>

<h2>Reinforcement Learning 介紹</h2>

<p>試想一個大學生，原本總是十二點睡，但離開家住進宿舍後，每天打電動打到兩點才睡，隔天上課昏昏沉沉，GPA 0.87。某天他嘗試十點上床睡覺，發現隔天上課腦袋清晰、神采奕奕，全身舒爽的他開始慢慢調整作息，最終 GPA 4.3。從對一個新環境一無所知，不斷嘗試不同作息時間，進而藉由所獲得的好處（身心感受、GPA等）學會最適合自己的作息，這種學習過程便是 Reinforcement Learning。</p>

<p>Reinforcement Learning 是 Machine Learning 家族的一員，為一種<strong>目標導向(goal-oriented)</strong>的學習方法，旨在經由與環境互動過程中獲得的各種獎勵或懲罰，學會如何做決策。</p>

<p><img src="https://i.stack.imgur.com/eoeSq.png" alt="reinforcement learning"></p>

<p>整個決策過程的模擬有以下幾個要素：</p>

<ol>
<li><strong>Agent</strong>，藉由 action 跟 environment 互動。</li>
<li><strong>Environment</strong>，agent 的行動範圍，根據 agent 的 action 給予不同程度的 reward。</li>
<li><strong>State</strong>，在特定時間點 agent 身處的狀態。</li>
<li><strong>Action</strong>，agent 藉由自身 policy 進行的動作。 </li>
<li><strong>Reward</strong>，environment 給予 agent 所做 action 的獎勵或懲罰。</li>
</ol>

<p>Agent 的目標是藉由與 environment 不斷互動及獲得 reward，學會最佳 <strong>policy</strong>，即是 agent 根據身處的 state 決定進行最佳 action 的策略。</p>

<p>以上是 Reinforcement Learning 的簡單介紹，欲深入了解可參考文末參考資料。</p>

<h2>OpenAI Gym 介紹</h2>

<p><a href="https://gym.openai.com/">OpenAI Gym</a> 是由 OpenAI 開源的 Reinforcement Learning 工具包，裡面有許多現成 environment 處理環境模擬及獎勵等等過程，讓開發者專注於演算法開發。</p>

<p><a href="https://gym.openai.com/docs/">安裝過程</a>非常簡單，首先確保你的 Python version 在 3.5 以上，然後使用 pip 安裝：</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">$ </span>pip install gym
</code></pre></div>
<p>接著只需要 <code>import gym</code> 就能開始體驗 Reinforcement Learning。</p>

<h2>演算法實作</h2>

<p><code>Gym</code> 一系列的 environment 都在<a href="https://gym.openai.com/envs/#classic_control">這裡</a>。我們挑選 <code>CartPole-v0</code> 當示範，任務是維持小車上的柱子的平衡。它的 environment 只有四種 feature（小車位置，小車速度，柱子角度，柱尖速度），agent 只有兩種 action（向左移，向右移）。網路上有非常多建立在 CartPole 的範例，這邊把常見演算法整合，進階的 Deep Q-Network 則留到下一篇。</p>

<p><img src="https://github.com/pyliaorachel/openai-gym-cartpole/blob/master/img/cartpole.png?raw=true" alt="cart pole"></p>

<p>GitHub 完整程式碼：<a href="https://github.com/pyliaorachel/openai-gym-cartpole">https://github.com/pyliaorachel/openai-gym-cartpole</a></p>

<h3>Random Action</h3>

<p>首先用最簡單的例子體驗 <code>gym</code> 的使用 —— 無論 environment 如何，隨機進行 action，也就是隨機決定要將小車左移或右移。</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s">'CartPole-v0'</span><span class="p">)</span>

<span class="c"># 跑 200 個 episode，每個 episode 都是一次任務嘗試</span>
<span class="k">for</span> <span class="n">i_episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">observation</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span> <span class="c"># 讓 environment 重回初始狀態 </span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="mi">0</span> <span class="c"># 累計各 episode 的 reward </span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">250</span><span class="p">):</span> <span class="c"># 設個時限，每個 episode 最多跑 250 個 action</span>
        <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span> <span class="c"># 呈現 environment</span>

        <span class="c"># Key section</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span> <span class="c"># 在 environment 提供的 action 中隨機挑選</span>
        <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="c"># 進行 action，environment 返回該 action 的 reward 及前進下個 state</span>

        <span class="n">rewards</span> <span class="o">+=</span> <span class="n">reward</span> <span class="c"># 累計 reward</span>

        <span class="k">if</span> <span class="n">done</span><span class="p">:</span> <span class="c"># 任務結束返回 done = True</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'Episode finished after {} timesteps, total rewards {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">rewards</span><span class="p">))</span>
            <span class="k">break</span>

<span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div>
<p>精華都在 Key section，agent 選擇並進行一個 action，並從 environment 中獲得 reward。可以看到 agent 並沒有任何學習行為，所以整體 reward 並不高。</p>

<p><img src="https://github.com/pyliaorachel/openai-gym-cartpole/blob/master/img/random_action.png?raw=true" alt="random action"></p>

<h3>Hand-Made Policy</h3>

<p>為了讓 agent 不會走得太無腦，再來引進一個簡單的 policy —— 如果柱子向左傾（角度 &lt; 0），則小車左移以維持平衡，否則右移。</p>

<blockquote>
<p>其實正常來說，agent 是不會知道 environment 所提供的這些 feature 和 action 各自是什麼意思，因此這一部分主要是示範 policy 的概念。</p>
</blockquote>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># 定義 policy</span>
<span class="k">def</span> <span class="nf">choose_action</span><span class="p">(</span><span class="n">observation</span><span class="p">):</span>
    <span class="n">pos</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">ang</span><span class="p">,</span> <span class="n">rot</span> <span class="o">=</span> <span class="n">observation</span>
    <span class="k">return</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">ang</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span> <span class="c"># 柱子左傾則小車左移，否則右移 </span>
</code></pre></div><div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s">'CartPole-v0'</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i_episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">observation</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">250</span><span class="p">):</span>
        <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>

        <span class="n">action</span> <span class="o">=</span> <span class="n">choose_action</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
        <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="n">rewards</span> <span class="o">+=</span> <span class="n">reward</span>

        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'Episode finished after {} timesteps, total rewards {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">rewards</span><span class="p">))</span>
            <span class="k">break</span>

<span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div>
<p>結果：</p>

<p><img src="https://github.com/pyliaorachel/openai-gym-cartpole/blob/master/img/hand_made_policy.png?raw=true" alt="hand-made rule"></p>

<p>可以看到 agent 所獲得的整體 reward 比 random action 高出許多，不過 agent 依然沒有根據經驗做學習，非常頑固。接下來要來示範真正實用的 learning process —— Q-learning。</p>

<h3>Q-Learning with Q Table</h3>

<p>在進入實作前，會先簡單講解 Q-learning 及 Q table 概念，可當作補充。</p>

<h6>Q-Learning</h6>

<p>為了學習在某個 state 之下做出好的 action，我們定義所謂的 Q function <code>Q(s, a)</code>，也就是<strong>根據身處的 state <code>s</code> 進行 action <code>a</code> 所預期未來會得到的總 reward</strong>。如果能求出最佳 Q function<code>Q*(s, a)</code>，我們的 agent 在任何 state 之下，只要挑選能最大化未來總 reward 的 action ，即 <code>argmax_a Q*(s, a)</code>，即能在任務中獲得最大 reward。而習得 Q function 的過程正是 Q-learning。</p>

<p>在學習 Q function 前，要先知道如何表示 Q function。不難發現 Q function 有遞迴特質，可以用遞迴表示：</p>

<p style="text-align: center;">
    <img src="https://cdn-images-1.medium.com/max/1600/1*jamiG5MkFVHLTFmLggemVg.png" />
</p>

<p>即是當前 reward 和進入下一個 state <code>s&#39;</code> 後所能獲得最大 discounted reward 的和。這邊的 γ 稱為 discount factor，可以說是對未來 reward 的重視程度。γ 越低，agent 越重視當前所獲得的 reward，並覺得未來獲得的 reward 太遙遠，不足以在當前 state 的決策過程中佔有太大份量。</p>

<p>接著 agent 要藉由一次次跟 environment 互動中獲得的 reward 來學習 Q function。起初 agent 一無所知時，Q function 的參數都是隨機的。接著從跟 environment 互動的每一步，慢慢更新參數，逼近我們要的最佳 Q function：</p>

<p style="text-align: center;">
    <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/845e1915e9fc3b55a3e88cb6fb5f40a41c1b6606" />
</p>

<p>這裡 learned value 是每次 action 帶來的一點新資訊，但不能直接取代舊資訊，而是每次更新 α 這麼多比率的新資訊，保留 (1 - α) 比率的舊資訊，最終逐漸收斂。</p>

<p>整體 Q-learning 步驟大致上如下：</p>

<p style="text-align: center;">
    <img src="http://www.incompleteideas.net/book/ebook/pseudotmp9.png" />
</p>

<p>ε-greedy 是一種在 exploration 和 exploitation 間取得平衡的方法。Exploration 是讓 agent 大膽嘗試不同 action，確保能夠吸收新知，而 exploitation 是讓 agent 保守沿用現有 policy，讓學習過程收斂。方法很簡單：ε 是隨機選擇 action 的機率，所以平均上有 ε 的時間 agent 會嘗試新 action，而 (1 - ε) 的時間 agent 會根據現有 policy 做決策。</p>

<h6>Q Table</h6>

<p>了解 Q-learning 及 ε-greedy 的概念後，那實際上這個 Q function 存在哪裡呢？一個樸實無華的方法就是把各個 state-action pair 的 Q value 存在 table 裡，直接查找或更新，即是所謂 Q table，也是接下來要示範的方法。不過這個方法的壞處是 table 大小有限，不適用於 state 和 action 過多的任務。</p>

<p>另一個方法是用 neural network 去逼近 Q function，即 Deep Q-Learning，如此一來就不會有容量限制了。這個方法會在之後另寫文章介紹。</p>

<h6>實作</h6>

<p>原始碼修改自<a href="https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947">這篇文章</a>。</p>

<p>先統整一下。我們的目標是學習到最佳 Q function，過程中以 ε-greedy 方法與 environment 互動，從中獲得 reward 以更新 Q table 裡的 Q value。先看一下基於 ε-greedy 的 policy 定義：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">q_table</span><span class="p">,</span> <span class="n">action_space</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_sample</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span> <span class="c"># 有 ε 的機率會選擇隨機 action</span>
        <span class="k">return</span> <span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span> 
    <span class="k">else</span><span class="p">:</span> <span class="c"># 其他時間根據現有 policy 選擇 action，也就是在 Q table 裡目前 state 中，選擇擁有最大 Q value 的 action</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">])</span> 
</code></pre></div>
<p>再來是 state 的表示。在 <code>CartPole</code> 環境裡觀察到的 feature 都是連續值，不適合作為一個 table 的 index，因此要將一個區間一個區間的值包在一起用離散數值表示，也就是下面的 <code>bucket</code>：</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">get_state</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="n">n_buckets</span><span class="p">,</span> <span class="n">state_bounds</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span> 
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">observation</span><span class="p">):</span> <span class="c"># 每個 feature 有不同的分配</span>
        <span class="n">l</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">state_bounds</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">state_bounds</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="c"># 每個 feature 值的範圍上下限</span>
        <span class="k">if</span> <span class="n">s</span> <span class="o">&lt;=</span> <span class="n">l</span><span class="p">:</span> <span class="c"># 低於下限，分配為 0</span>
            <span class="n">state</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="n">s</span> <span class="o">&gt;=</span> <span class="n">u</span><span class="p">:</span> <span class="c"># 高於上限，分配為最大值</span>
            <span class="n">state</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_buckets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span> <span class="c"># 範圍內，依比例分配</span>
            <span class="n">state</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(((</span><span class="n">s</span> <span class="o">-</span> <span class="n">l</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="n">l</span><span class="p">))</span> <span class="o">*</span> <span class="n">n_buckets</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
</code></pre></div>
<p>最後是學習。學習過程中為了方便收斂，一些參數像 ε 和 learning rate 會隨著時間遞減，也就是我們從大膽亂走，到越來越相信已經學到的經驗。</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s">'CartPole-v0'</span><span class="p">)</span>

<span class="c"># 準備 Q table</span>
<span class="c">## Environment 中各個 feature 的 bucket 分配數量</span>
<span class="c">## 1 代表任何值皆表同一 state，也就是這個 feature 其實不重要</span>
<span class="n">n_buckets</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c">## Action 數量 </span>
<span class="n">n_actions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>

<span class="c">## State 範圍 </span>
<span class="n">state_bounds</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">low</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">high</span><span class="p">))</span>
<span class="n">state_bounds</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="n">state_bounds</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span> <span class="n">math</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="mi">50</span><span class="p">)]</span>

<span class="c">## Q table，每個 state-action pair 存一值 </span>
<span class="n">q_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_buckets</span> <span class="o">+</span> <span class="p">(</span><span class="n">n_actions</span><span class="p">,))</span>

<span class="c"># 一些學習過程中的參數</span>
<span class="n">get_epsilon</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">math</span><span class="o">.</span><span class="n">log10</span><span class="p">((</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">25</span><span class="p">)))</span>  <span class="c"># epsilon-greedy; 隨時間遞減</span>
<span class="n">get_lr</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">math</span><span class="o">.</span><span class="n">log10</span><span class="p">((</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">25</span><span class="p">)))</span> <span class="c"># learning rate; 隨時間遞減 </span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span> <span class="c"># reward discount factor</span>

<span class="c"># Q-learning</span>
<span class="k">for</span> <span class="n">i_episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">get_epsilon</span><span class="p">(</span><span class="n">i_episode</span><span class="p">)</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">get_lr</span><span class="p">(</span><span class="n">i_episode</span><span class="p">)</span>

    <span class="n">observation</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">get_state</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="n">n_buckets</span><span class="p">,</span> <span class="n">state_bounds</span><span class="p">)</span> <span class="c"># 將連續值轉成離散 </span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">250</span><span class="p">):</span>
        <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>

        <span class="n">action</span> <span class="o">=</span> <span class="n">choose_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">q_table</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
        <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="n">rewards</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">get_state</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="n">n_buckets</span><span class="p">,</span> <span class="n">state_bounds</span><span class="p">)</span>

        <span class="c"># 更新 Q table</span>
        <span class="n">q_next_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">q_table</span><span class="p">[</span><span class="n">next_state</span><span class="p">])</span> <span class="c"># 進入下一個 state 後，預期得到最大總 reward</span>
        <span class="n">q_table</span><span class="p">[</span><span class="n">state</span> <span class="o">+</span> <span class="p">(</span><span class="n">action</span><span class="p">,)]</span> <span class="o">+=</span> <span class="n">lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">q_next_max</span> <span class="o">-</span> <span class="n">q_table</span><span class="p">[</span><span class="n">state</span> <span class="o">+</span> <span class="p">(</span><span class="n">action</span><span class="p">,)])</span> <span class="c"># 就是那個公式</span>

        <span class="c"># 前進下一 state </span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'Episode finished after {} timesteps, total rewards {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">rewards</span><span class="p">))</span>
            <span class="k">break</span>

<span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div>
<p>這邊其實偷偷作弊，才會知道哪個 feature 重要哪個不重要，以及 state 的上下限。參數也是原作者調整過的。不過如此一來才能展現好結果：</p>

<p><img src="https://github.com/pyliaorachel/openai-gym-cartpole/blob/master/img/q_table.png?raw=true" alt="Q table"></p>

<p>可以看到在訓練後期，agent 已經學會如何最大化自己的 reward，也就是維持住小車上的棒子了。</p>

<h2>結語</h2>

<p>AlphaGo 帶來的驚奇讓人們期待著 Reinforcement Learning 的無限可能性。此篇帶大家簡單理解 Reinforcement Learning 的學習過程和 OpenAI Gym 的操作，並簡單示範幾個演算法。雖說目前 Reinforcement Learning 打造出許多超越人類的遊戲 AI，但在其他領域的應用，例如 Computer Vision、Natural Language Processing，仍成果有限。期待不久的未來，Reinforcement Learning 能在真正對人類福祉有益的領域有所突破。</p>

<h2>參考資料</h2>

<ul>
<li><a href="https://www.nature.com/articles/nature24270">Mastering the game of Go without human knowledge</a></li>
<li><a href="https://deeplearning4j.org/deepreinforcementlearning">A Beginner&#39;s Guide to Deep Reinforcement Learning</a></li>
<li><a href="https://medium.com/@curiousily/solving-an-mdp-with-q-learning-from-scratch-deep-reinforcement-learning-for-hackers-part-1-45d1d360c120">Solving an MDP with Q-Learning from scratch — Deep Reinforcement Learning for Hackers (Part 1)</a></li>
<li><a href="https://blog.techbridge.cc/2017/11/04/openai-gym-intro-and-q-learning/">Open AI Gym 簡介與 Q learning 演算法實作</a></li>
<li><a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/4-02-RL-methods/">强化学习方法汇总 (Reinforcement Learning)</a></li>
<li><a href="https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947">Cart-Pole Balancing with Q-Learning</a></li>
</ul>

		</div>

		<ul class="tag-list">
			

			<li class="tag">
				Python
			</li>

			

			<li class="tag">
				PyLadies
			</li>

			

			<li class="tag">
				OpenAI gym
			</li>

			
		</ul>

	</article>

	
		<div id="disqus_thread"></div>
<script>
	var disqus_config = function () {
        this.page.url = "https://pyliaorachel.github.io/blog/tech/python/2018/06/01/openai-gym-for-reinforcement-learning.html";
        this.page.identifier = "/blog/tech/python/2018/06/01/openai-gym-for-reinforcement-learning";
	};
	(function() {
	    var d = document, s = d.createElement('script');
        s.src = '//pyliaorachel.disqus.com/embed.js';
	    s.setAttribute('data-timestamp', +new Date());
	    (d.head || d.body).appendChild(s);
	})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

	
</div>
	</div>
</main>
		<footer>
	<div class="container-fluid">
		<div class="row">
			<div class="col-md-4 footer-link">
				<a href="mailto:rachel5566kk@gmail.com">
					<span class="icon icon-email">
						<img src="/assets/svg/email-logo.svg" alt="Email Logo">
					</span>
					<span class="username">rachel5566kk@gmail.com</span>
				</a>
			</div>
			<div class="col-md-4 footer-link">
				<a href="https://github.com/pyliaorachel"  target="_blank">
					<span class="icon icon-github">
						<img src="/assets/svg/github-logo.svg" alt="GitHub Logo">
					</span>
					<span class="username">pyliaorachel</span>
				</a>
			</div>
			<div class="col-md-4 footer-link">
				<a href="https://tw.linkedin.com/in/peiyu-liao" target="_blank">
					<span class="icon icon-linkedin">
						<img src="/assets/svg/linkedin-logo.svg" alt="LinkedIn Logo">
					</span>
					<span class="username">Peiyu Liao</span>
				</a>
			</div>
		</div><!-- /.row -->
		<div class="row">
			<div class="footer-copy-right">
				<p>2018	&copy; Liao Peiyu</p>
			</div >
		</div><!-- /.row -->
	</div><!-- /.container-fluid -->
</footer>

		<script type="text/javascript" src="/js/main.js"></script>
		<script type="text/javascript" src="/js/home.js"></script>
        <script id="dsq-count-scr" src="//pyliaorachel.disqus.com/count.js" async></script>
	</body>
</html>
